{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed660ca",
   "metadata": {
    "papermill": {
     "duration": 0.011852,
     "end_time": "2024-01-22T10:00:26.836871",
     "exception": false,
     "start_time": "2024-01-22T10:00:26.825019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**This notebook is just a slight modification of the below notebook**(Hyperparam tuning)\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/albansteff/enefit-estonian-holidays-lb-65-79\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9793a91e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:26.861223Z",
     "iopub.status.busy": "2024-01-22T10:00:26.860535Z",
     "iopub.status.idle": "2024-01-22T10:00:36.411312Z",
     "shell.execute_reply": "2024-01-22T10:00:36.410452Z"
    },
    "papermill": {
     "duration": 9.565882,
     "end_time": "2024-01-22T10:00:36.414037",
     "exception": false,
     "start_time": "2024-01-22T10:00:26.848155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, joblib, gc, pickle\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import catboost as cbt \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import ctypes;\n",
    "libc = ctypes.CDLL('libc.so.6');\n",
    "from tqdm import tqdm\n",
    "import holidays\n",
    "import datetime \n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c6707",
   "metadata": {
    "papermill": {
     "duration": 0.01073,
     "end_time": "2024-01-22T10:00:36.436023",
     "exception": false,
     "start_time": "2024-01-22T10:00:36.425293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4b3577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:36.460929Z",
     "iopub.status.busy": "2024-01-22T10:00:36.459803Z",
     "iopub.status.idle": "2024-01-22T10:00:37.561621Z",
     "shell.execute_reply": "2024-01-22T10:00:37.560040Z"
    },
    "papermill": {
     "duration": 1.117616,
     "end_time": "2024-01-22T10:00:37.564581",
     "exception": false,
     "start_time": "2024-01-22T10:00:36.446965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755510a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.590333Z",
     "iopub.status.busy": "2024-01-22T10:00:37.589285Z",
     "iopub.status.idle": "2024-01-22T10:00:37.594970Z",
     "shell.execute_reply": "2024-01-22T10:00:37.594039Z"
    },
    "papermill": {
     "duration": 0.021471,
     "end_time": "2024-01-22T10:00:37.597131",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.575660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_models = '/kaggle/input/models-enefit/'\n",
    "path_data = '/kaggle/input/dataset-enefit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8427464b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.621604Z",
     "iopub.status.busy": "2024-01-22T10:00:37.620920Z",
     "iopub.status.idle": "2024-01-22T10:00:37.625832Z",
     "shell.execute_reply": "2024-01-22T10:00:37.624824Z"
    },
    "papermill": {
     "duration": 0.019872,
     "end_time": "2024-01-22T10:00:37.628179",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.608307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_models = 1\n",
    "n_estimators = 2500\n",
    "device ='cpu'\n",
    "l_ca = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595d97bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.653037Z",
     "iopub.status.busy": "2024-01-22T10:00:37.652584Z",
     "iopub.status.idle": "2024-01-22T10:00:37.657875Z",
     "shell.execute_reply": "2024-01-22T10:00:37.656756Z"
    },
    "papermill": {
     "duration": 0.021145,
     "end_time": "2024-01-22T10:00:37.660196",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.639051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "sequence_length = 7\n",
    "batch_size = 1024 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4cf8b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.684845Z",
     "iopub.status.busy": "2024-01-22T10:00:37.683742Z",
     "iopub.status.idle": "2024-01-22T10:00:37.690082Z",
     "shell.execute_reply": "2024-01-22T10:00:37.689191Z"
    },
    "papermill": {
     "duration": 0.021105,
     "end_time": "2024-01-22T10:00:37.692407",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.671302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_feature = ['cloudcover_high_historical_168h', 'cloudcover_mid_historical_local_168h', 'cloudcover_mid_historical_24h', 'snowfall_historical_48h', 'rain_historical_168h', 'cloudcover_high_historical_local_168h', 'temperature_historical_24h', 'windspeed_10m_historical_local_168h', 'cloudcover_total_forecast_local_168h', 'dewpoint_historical_48h', 'cloudcover_high_historical_48h', 'surface_pressure_historical_24h', 'surface_pressure_historical_local_48h', 'cloudcover_mid_forecast_local_168h', 'winddirection_10m_historical_168h', 'cloudcover_low_historical_168h', '10_metre_v_wind_component_forecast_local_0h', 'cloudcover_total_forecast_168h', 'cloudcover_total_historical_168h', 'target_all_county_type_sum_ratio_168_336', 'winddirection_10m_historical_local_168h', 'direct_solar_radiation_historical_24h']\n",
    "l1=['installed_capacity', 'target_mean', 'target_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166c9ec",
   "metadata": {
    "papermill": {
     "duration": 0.010937,
     "end_time": "2024-01-22T10:00:37.714665",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.703728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### DataStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c378daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.739356Z",
     "iopub.status.busy": "2024-01-22T10:00:37.738145Z",
     "iopub.status.idle": "2024-01-22T10:00:37.767178Z",
     "shell.execute_reply": "2024-01-22T10:00:37.766064Z"
    },
    "papermill": {
     "duration": 0.044648,
     "end_time": "2024-01-22T10:00:37.770294",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.725646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "    data_cols = [\"target\",\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",\"row_id\",]\n",
    "    client_cols = [\"product_type\",\"county\",\"eic_count\",\"installed_capacity\",\"is_business\",\"date\",]\n",
    "    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_cols = [\"latitude\",\"longitude\",\"hours_ahead\",\"temperature\",\"dewpoint\",\"cloudcover_high\",\"cloudcover_low\",\"cloudcover_mid\",\"cloudcover_total\",\"10_metre_u_wind_component\",\"10_metre_v_wind_component\",\"forecast_datetime\",\"direct_solar_radiation\",\"surface_solar_radiation_downwards\",\"snowfall\",\"total_precipitation\",]\n",
    "    historical_weather_cols = [\"datetime\",\"temperature\",\"dewpoint\",\"rain\",\"snowfall\",\"surface_pressure\",\"cloudcover_total\",\"cloudcover_low\",\"cloudcover_mid\",\"cloudcover_high\",\"windspeed_10m\",\"winddirection_10m\",\"shortwave_radiation\",    \"direct_solar_radiation\",\"diffuse_radiation\",\"latitude\",\"longitude\",]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\"target\",\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(os.path.join(self.root, \"train.csv\"),columns=self.data_cols,try_parse_dates=True,)\n",
    "        self.df_client = pl.read_csv(os.path.join(self.root, \"client.csv\"),columns=self.client_cols,try_parse_dates=True,)\n",
    "        self.df_gas_prices = pl.read_csv(os.path.join(self.root, \"gas_prices.csv\"),columns=self.gas_prices_cols,try_parse_dates=True,)\n",
    "        self.df_electricity_prices = pl.read_csv(os.path.join(self.root, \"electricity_prices.csv\"),columns=self.electricity_prices_cols,try_parse_dates=True,)\n",
    "        self.df_forecast_weather = pl.read_csv(os.path.join(self.root, \"forecast_weather.csv\"),columns=self.forecast_weather_cols,try_parse_dates=True,)\n",
    "        self.df_historical_weather = pl.read_csv(os.path.join(self.root, \"historical_weather.csv\"),columns=self.historical_weather_cols,try_parse_dates=True,)\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),columns=self.location_cols,try_parse_dates=True,)\n",
    "        self.df_data = self.df_data.filter(pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\"))\n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (self.df_weather_station_to_county_mapping.with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),))\n",
    "        self.max_date_data = self.df_data['datetime'].max()\n",
    "\n",
    "    def update_with_new_data(self,df_new_client,df_new_gas_prices,df_new_electricity_prices,df_new_forecast_weather,df_new_historical_weather,df_new_target,):\n",
    "        df_new_client = pl.from_pandas(df_new_client[self.client_cols], schema_overrides=self.schema_client)\n",
    "        df_new_gas_prices = pl.from_pandas(df_new_gas_prices[self.gas_prices_cols],schema_overrides=self.schema_gas_prices,)\n",
    "        df_new_electricity_prices = pl.from_pandas(df_new_electricity_prices[self.electricity_prices_cols],schema_overrides=self.schema_electricity_prices,)\n",
    "        df_new_forecast_weather = pl.from_pandas(df_new_forecast_weather[self.forecast_weather_cols],schema_overrides=self.schema_forecast_weather,)\n",
    "        df_new_historical_weather = pl.from_pandas(df_new_historical_weather[self.historical_weather_cols],schema_overrides=self.schema_historical_weather,)\n",
    "        df_new_target = pl.from_pandas(df_new_target[self.target_cols], schema_overrides=self.schema_target)\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique([\"date\", \"county\", \"is_business\", \"product_type\"])\n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique([\"forecast_date\"])\n",
    "        self.df_electricity_prices = pl.concat([self.df_electricity_prices, df_new_electricity_prices]).unique([\"forecast_date\"])\n",
    "        self.df_forecast_weather = pl.concat([self.df_forecast_weather, df_new_forecast_weather]).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat([self.df_historical_weather, df_new_historical_weather]).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique([\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"])\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(df_test[self.data_cols[1:]], schema_overrides=self.schema_data)\n",
    "        return df_test\n",
    "    \n",
    "    def filter_on_test(self, df_test):\n",
    "        df_test = df_test.filter(df_test['datetime'] > self.max_date_data)\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c1ea0",
   "metadata": {
    "papermill": {
     "duration": 0.011147,
     "end_time": "2024-01-22T10:00:37.793042",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.781895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### FeaturesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd81851e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.817645Z",
     "iopub.status.busy": "2024-01-22T10:00:37.817209Z",
     "iopub.status.idle": "2024-01-22T10:00:37.925076Z",
     "shell.execute_reply": "2024-01-22T10:00:37.923989Z"
    },
    "papermill": {
     "duration": 0.123995,
     "end_time": "2024-01-22T10:00:37.928188",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.804193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeaturesGenerator:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = holidays.country_holidays('EE', years=range(2021, 2026))\n",
    "        self.first = True\n",
    "        self.hours_list= [i * 24 for i in range(2,15)]\n",
    "        self.all_targets_laged = [ f\"target_{hours_lag}h\" for hours_lag in self.hours_list]\n",
    "        self.cols_for_stats = [ f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24, 7 * 24]]\n",
    "        \n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.date().alias(\"mdate\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\"county\",\"is_business\",\"product_type\",\"is_consumption\",separator=\"_\",).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns((pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])in self.estonian_holidays)\n",
    "\n",
    "    def _holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            #.drop(\"hours_ahead\")\n",
    "            .with_columns(pl.col(\"latitude\").cast(pl.datatypes.Float32),pl.col(\"longitude\").cast(pl.datatypes.Float32),)\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\"))\n",
    "\n",
    "        df_forecast_weather_local = (df_forecast_weather.filter(pl.col(\"county\").is_not_null()).group_by(\"county\", \"datetime\").mean())\n",
    "\n",
    "        for hours_lag in [0,  2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (self.data_storage.df_weather_station_to_county_mapping)\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\"))\n",
    "        df_target_all_county_type_sum = (df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"]).sum().drop(\"product_type\", \"county\"))\n",
    "\n",
    "        for hours_lag in self.hours_list:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\"county\",\"is_business\",\"product_type\",\"is_consumption\",\"datetime\",], how=\"left\",)\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join( df_target_all_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"], how=\"left\",)\n",
    "\n",
    "            df_features = df_features.join( df_target_all_county_type_sum.with_columns(pl.col(\"datetime\") + pl.duration(hours=hours_lag)).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"], how=\"left\", suffix=f\"_all_county_type_sum_{hours_lag}h\", )\n",
    "\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(self.cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(self.cols_for_stats).transpose().std().transpose().to_series().alias(f\"target_std\"),\n",
    "            df_features.select(self.all_targets_laged).mean(axis=1).alias(f\"all_target_mean\"),\n",
    "            df_features.select(self.all_targets_laged).transpose().std().transpose().to_series().alias(f\"all_target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (pl.col(f\"{target_prefix}_{lag_nominator}h\")/ (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop([\"date\", \"datetime\", \"hour\", \"dayofyear\", 'mdate', 'literal',\n",
    "                                        'year', 'log_target_all_county_type_sum_ratio_168_336',\n",
    "                                        'windspeed_10m_historical_local_48h', 'cloudcover_low_historical_168h', 'total_precipitation_forecast_168h', 'cloudcover_high_forecast_168h', 'cloudcover_low_historical_local_168h', 'surface_pressure_historical_local_48h', '10_metre_u_wind_component_forecast_48h', 'log_winddirection_10m_historical_168h', 'log_cloudcover_low_historical_168h', 'cloudcover_high_historical_168h', 'surface_pressure_historical_168h', '10_metre_u_wind_component_forecast_local_0h', 'windspeed_10m', 'target_48h_max_day_county', 'cloudcover_high_historical_local_168h', 'cloudcover_low_forecast_168h', 'mean_3_ssrdfl0_mean_day', 'log_cloudcover_total_forecast_168h', 'rain', 'mean_5_target_48h_mean_day', 'log_cloudcover_total_historical_168h', 'target_48h_std_day_county', 'cloudcover_mid_forecast_local_168h', 'rain_historical_168h', 'mean_3_target_48h_mean_day', 'total_precipitation_forecast_48h', 'temperature_historical_24h', '10_metre_u_wind_component_forecast_local_48h', 'log_cloudcover_mid_forecast_local_168h', 'dewpoint_historical_48h', 'ssrdfl0_min_day_county', 'cloudcover_high_historical_24h', 'windspeed_10m_historical_168h', 'snowfall_forecast_48h', 'std_3_target_48h', 'mean_5_target_48h', '10_metre_v_wind_component_diff_1', 'rain_historical_local_168h', 'winddirection_10m', 'temperature_forecast_48h', 'log_cloudcover_high_historical_local_168h', 'z_score_5_ssrdfl0_mean_day', 'z_score_3_ssrdfl0_mean_day', 'dewpoint_historical_local_48h', 'cloudcover_high_forecast_48h', 'snowfall_historical_48h', 'snowfall_forecast_local_48h', 'log_cloudcover_high_historical_168h', 'mean_3_target_48h', 'log_windspeed_10m_historical_local_168h', 'std_5_target_48h_mean_day', 'surface_pressure', 'mean_5_target_48h_sr', 'log_cloudcover_total_forecast_local_168h', 'std_3_target_48h_mean_day', 'std_3_ssrdfl0_mean_day', 'z_score_5_target_48h_mean_day',\n",
    "                                        'snowfall_forecast_168h', 'mean_5_ssrdfl0_mean_day', 'log_temperature_historical_24h', 'winddirection_10m_historical_local_48h', 'total_precipitation_forecast_local_48h', 'log_surface_pressure_historical_24h', 'cloudcover_high_forecast_local_48h', 'winddirection_10m_historical_24h', 'windspeed_10m_historical_local_168h', 'dewpoint_historical_24h', 'dewpoint_forecast_48h', 'std_5_target_48h', 'snowfall_historical_local_48h',\n",
    "                                        'cloudcover_mid_historical_168h', 'cloudcover_total_historical_local_168h', 'cloudcover_mid_historical_local_168h', 'winddirection_10m_historical_local_168h', '10_metre_v_wind_component_forecast_local_168h', 'cloudcover_total_forecast_168h', '10_metre_v_wind_component_forecast_48h', 'std_5_ssrdfl0_mean_day', 'log_dewpoint_historical_48h', 'log_winddirection_10m_historical_local_168h', 'z_score_3_target_48h', 'kurt', 'snowfall_forecast_local_168h',\n",
    "                                        'hours_ahead_forecast_168h', 'hours_ahead_forecast_local_168h',\n",
    "                                       ], axis = 1 , errors='ignore')\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[l_ca] = df_features[l_ca].astype(\"category\")\n",
    "\n",
    "        #df_features = df_features.drop('segment', axis = 1)\n",
    "        return df_features\n",
    "    \n",
    "    # added some new features here\n",
    "    def _additional_features(self,df):        \n",
    "        #####################################################\n",
    "        df['feature1'] = df['installed_capacity'] * df['surface_solar_radiation_downwards'] / (df['temperature'] + 273.15)\n",
    "        df['feature2'] = df['installed_capacity'] * df['surface_solar_radiation_downwards'] / df['total_precipitation']\n",
    "        df['skew'] = df[self.cols_for_stats].skew(axis = 1)\n",
    "        df['kurt'] = df[self.cols_for_stats].kurt(axis = 1)\n",
    "        df['skew2'] = df[self.all_targets_laged].skew(axis = 1)\n",
    "        df['kurt2'] = df[self.all_targets_laged].kurt(axis = 1)\n",
    "        df['target_mean_r_all_target_mean'] = df['target_mean'] / df['all_target_mean']\n",
    "        df['target_std_r_all_target_std'] = df['target_std'] / df['all_target_std']\n",
    "        df['target_mean_sr'] = df['target_mean'] / df['target_std']\n",
    "        df['all_target_sr'] = df['all_target_mean'] / df['all_target_std']\n",
    "        \n",
    "        for col in ['temperature', 'dewpoint', '10_metre_u_wind_component', '10_metre_v_wind_component', 'target_48h', 'feature1',  'target_168h']:\n",
    "            for window in [1]:\n",
    "                df[f\"{col}_diff_{window}\"] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])[col].diff(window)\n",
    "                \n",
    "        df['feature1_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"feature1_diff_1\"].transform('mean')\n",
    "        df['feature1_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"feature1_diff_1\"].transform('std')\n",
    "        \n",
    "        df['target_48h_diff_1_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h_diff_1\"].transform('mean')\n",
    "        df['target_48h_diff_1_mean_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business', 'year', 'day', 'month'])[\"target_48h_diff_1\"].transform('mean')\n",
    "        df['target_48h_diff_1_std_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h_diff_1\"].transform('std')\n",
    "        df['target_48h_diff_1_std_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business', 'year', 'day', 'month'])[\"target_48h_diff_1\"].transform('std')\n",
    "\n",
    "        df['target_48h_m_target_72h'] = df['target_48h'] - df['target_72h']\n",
    "        df['target_48h_m_target_96h'] = df['target_48h'] - df['target_96h']   \n",
    "        df['target_48h_m_target_169h'] = df['target_48h'] - df['target_168h']   \n",
    "        df['target_48h_m_target_336h'] = df['target_48h'] - df['target_336h']    \n",
    "        df['target_48h_m_target_mean'] = df['target_48h'] - df['target_mean']\n",
    "\n",
    "        df['accel_3'] = df['target_48h_m_target_96h'] - df['target_48h_m_target_mean']\n",
    "        df['z_score_target_48h'] = ((df['target_48h'] - df['target_mean']) / df['target_std']).fillna(0)\n",
    "                \n",
    "        df['mean_target_48h_72h'] = (df['target_48h'] + df['target_72h']) / 2\n",
    "        df['mean_target_48h_72h_96h'] = (df['target_48h'] + df['target_72h'] + df['target_96h']) / 3\n",
    "\n",
    "        df['target_48h_m_mean_target_48h_72h'] = df['target_48h'] - df['mean_target_48h_72h']\n",
    "        df['z_score_1_target_48h'] = ((df['target_48h'] - df['target_48h_m_mean_target_48h_72h']) / df[['target_48h','target_72h']].std(axis = 1)).fillna(0)\n",
    "\n",
    "        df['target_48h_m_mean_target_48h_72h_96h'] = df['target_48h'] - df['mean_target_48h_72h_96h']\n",
    "        df['m_std_target_48h_72h_96h'] = df[['target_48h','target_72h','target_96h']].std(axis = 1)\n",
    "        df['z_score_2_target_48h'] = ((df['target_48h'] - df['target_48h_m_mean_target_48h_72h_96h']) / df['m_std_target_48h_72h_96h']).fillna(0)\n",
    "\n",
    "        df['diff_dsrfl'] =  df['direct_solar_radiation_forecast_local_0h'] / df['direct_solar_radiation_forecast_local_168h']\n",
    "        df['diff_ssrdfl'] =  df['surface_solar_radiation_downwards_forecast_local_0h'] / df['surface_solar_radiation_downwards_forecast_local_168h']\n",
    "        df['ratio_target48_ic'] =  df['target_48h'] / df['installed_capacity']\n",
    "        df['ratio_target_168h_ic'] =  df['target_168h'] / df['installed_capacity']\n",
    "        df['ratio_target_mean_ic'] =  df['target_mean'] / df['installed_capacity']\n",
    "        \n",
    "        if self.first :\n",
    "            self.first = False\n",
    "            self.dic_min ={}\n",
    "            for col in log_feature[:]:\n",
    "                self.dic_min[col] = min(df[col])\n",
    "\n",
    "        for col in log_feature:\n",
    "            df[f'log_{col}'] = np.where((df[col] - self.dic_min[col])!= 0, np.log(df[col] - self.dic_min[col]),0)\n",
    "            \n",
    "        #####################################################\n",
    "        df['mean_3_target_48h'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h'].rolling(3).mean().values\n",
    "        df['mean_5_target_48h'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h'].rolling(5).mean().values\n",
    "        df['std_3_target_48h'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h'].rolling(3).std().values\n",
    "        df['std_5_target_48h'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h'].rolling(5).std().values\n",
    "        df['z_score_3_target_48h'] = ((df['target_48h'] - df['mean_3_target_48h']) / df['std_3_target_48h']).fillna(0)\n",
    "        df['z_score_5_target_48h'] = ((df['target_48h'] - df['mean_5_target_48h']) / df['std_5_target_48h']).fillna(0)\n",
    "        \n",
    "        df['mean_3_target_48h_sr'] =  df['mean_3_target_48h'] / df['std_3_target_48h']\n",
    "        df['mean_5_target_48h_sr'] =  df['mean_5_target_48h'] / df['std_5_target_48h']\n",
    "        \n",
    "        #####################################################\n",
    "        df['target_48h_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('mean')\n",
    "        df['target_48h_mean_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('mean')\n",
    "        \n",
    "        df['target_48h_min_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('min')\n",
    "        df['target_48h_min_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('min')\n",
    "                \n",
    "        df['target_48h_max_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('max')\n",
    "        df['target_48h_max_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('max')\n",
    "\n",
    "        df['target_48h_std_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('std')\n",
    "        df['target_48h_std_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"target_48h\"].transform('std')\n",
    "\n",
    "        df['target_48h_sr_day'] =  df['target_48h_mean_day'] / df['target_48h_std_day']\n",
    "        df['target_48h_sr_day_county'] =  df['target_48h_mean_day_county'] / df['target_48h_std_day_county']\n",
    "        \n",
    "        df['mean_3_target_48h_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h_mean_day'].rolling(3).mean().values\n",
    "        df['mean_5_target_48h_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h_mean_day'].rolling(5).mean().values\n",
    "        df['std_3_target_48h_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h_mean_day'].rolling(3).std().values\n",
    "        df['std_5_target_48h_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['target_48h_mean_day'].rolling(5).std().values\n",
    "        df['z_score_3_target_48h_mean_day'] = ((df['target_48h_mean_day'] - df['mean_3_target_48h_mean_day']) / df['std_3_target_48h_mean_day']).fillna(0)\n",
    "        df['z_score_5_target_48h_mean_day'] = ((df['target_48h_mean_day'] - df['mean_5_target_48h_mean_day']) / df['std_5_target_48h_mean_day']).fillna(0)\n",
    "        \n",
    "        #####################################################\n",
    "        df['ssrdfl0_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('mean')\n",
    "        df['ssrdfl0_mean_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('mean')\n",
    "        \n",
    "        df['feature3'] = df['installed_capacity'] * df['ssrdfl0_mean_day'] / (df['temperature'] + 273.15)\n",
    "        df['feature4'] = df['installed_capacity'] * df['ssrdfl0_mean_day_county'] / (df['temperature'] + 273.15)\n",
    "        \n",
    "        df['ssrdfl0_min_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('min')\n",
    "        df['ssrdfl0_min_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('min')\n",
    "                \n",
    "        df['ssrdfl0_max_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('max')\n",
    "        df['ssrdfl0_max_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('max')\n",
    "\n",
    "        df['ssrdfl0_std_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('std')\n",
    "        df['ssrdfl0_std_day_county'] = df.groupby(['is_consumption', 'product_type', 'is_business','year', 'day', 'month'])[\"surface_solar_radiation_downwards_forecast_local_0h\"].transform('std')\n",
    "\n",
    "        df['ssrdfl0_sr_day'] =  df['ssrdfl0_mean_day'] / df['ssrdfl0_std_day']\n",
    "        df['ssrdfl0_sr_day_county'] =  df['ssrdfl0_mean_day_county'] / df['ssrdfl0_std_day_county']\n",
    "        \n",
    "        df['mean_3_ssrdfl0_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['ssrdfl0_mean_day'].rolling(3).mean().values\n",
    "        df['mean_5_ssrdfl0_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['ssrdfl0_mean_day'].rolling(5).mean().values\n",
    "        df['std_3_ssrdfl0_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['ssrdfl0_mean_day'].rolling(3).std().values\n",
    "        df['std_5_ssrdfl0_mean_day'] = df.groupby([\"county\", 'is_consumption', 'product_type', 'is_business'])['ssrdfl0_mean_day'].rolling(5).std().values\n",
    "        df['z_score_3_ssrdfl0_mean_day'] = ((df['ssrdfl0_mean_day'] - df['mean_3_ssrdfl0_mean_day']) / df['std_3_target_48h_mean_day']).fillna(0)\n",
    "        df['z_score_5_ssrdfl0_mean_day'] = ((df['ssrdfl0_mean_day'] - df['mean_5_ssrdfl0_mean_day']) / df['std_5_target_48h_mean_day']).fillna(0)\n",
    "\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def _log_outliers(self,df):\n",
    "        for i in l1:\n",
    "            df = df.with_columns([(f\"log_outliers_{i}\", pl.when(df[i] != 0).then(np.log(pl.col(i))).otherwise(0))])\n",
    "        return df\n",
    "    \n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (df_prediction_items.drop(\"target\"),df_prediction_items.select(\"target\"),)\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),)\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._holidays_features,\n",
    "            self._log_outliers,\n",
    "            self._reduce_memory_usage\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        df_features = self._additional_features(df_features)\n",
    "        df_features = self._drop_columns(df_features)\n",
    "        \n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1065c75",
   "metadata": {
    "papermill": {
     "duration": 0.010827,
     "end_time": "2024-01-22T10:00:37.950046",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.939219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e5f4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:37.974506Z",
     "iopub.status.busy": "2024-01-22T10:00:37.973877Z",
     "iopub.status.idle": "2024-01-22T10:00:37.979158Z",
     "shell.execute_reply": "2024-01-22T10:00:37.978354Z"
    },
    "papermill": {
     "duration": 0.020169,
     "end_time": "2024-01-22T10:00:37.981527",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.961358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_mae(pred, target):\n",
    "    return np.sum(np.abs((pred - target)))/len(target)\n",
    "\n",
    "def feval_mae(y_pred, lgb_data2):\n",
    "    y_true = lgb_data2.get_label()\n",
    "    return 'mae', my_mae(y_pred, y_true), False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2154a",
   "metadata": {
    "papermill": {
     "duration": 0.010493,
     "end_time": "2024-01-22T10:00:38.002957",
     "exception": false,
     "start_time": "2024-01-22T10:00:37.992464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialisation & Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e50f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:38.027511Z",
     "iopub.status.busy": "2024-01-22T10:00:38.026802Z",
     "iopub.status.idle": "2024-01-22T10:00:44.159056Z",
     "shell.execute_reply": "2024-01-22T10:00:44.157952Z"
    },
    "papermill": {
     "duration": 6.147263,
     "end_time": "2024-01-22T10:00:44.161564",
     "exception": false,
     "start_time": "2024-01-22T10:00:38.014301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4dc0a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:00:44.185501Z",
     "iopub.status.busy": "2024-01-22T10:00:44.184874Z",
     "iopub.status.idle": "2024-01-22T10:01:50.165022Z",
     "shell.execute_reply": "2024-01-22T10:01:50.163741Z"
    },
    "papermill": {
     "duration": 65.995673,
     "end_time": "2024-01-22T10:01:50.168215",
     "exception": false,
     "start_time": "2024-01-22T10:00:44.172542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]\n",
    "df_train_features = df_train_features.drop([\"date\", \"hour\"], axis = 1 , errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac79d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:01:50.192378Z",
     "iopub.status.busy": "2024-01-22T10:01:50.191984Z",
     "iopub.status.idle": "2024-01-22T10:01:50.650785Z",
     "shell.execute_reply": "2024-01-22T10:01:50.649539Z"
    },
    "papermill": {
     "duration": 0.47437,
     "end_time": "2024-01-22T10:01:50.654085",
     "exception": false,
     "start_time": "2024-01-22T10:01:50.179715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 ['county', 'is_business', 'product_type', 'is_consumption', 'day', 'weekday', 'month', 'segment', 'sin(dayofyear)', 'cos(dayofyear)', 'sin(hour)', 'cos(hour)', 'eic_count', 'installed_capacity', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation', 'hours_ahead_forecast_local_0h', 'temperature_forecast_local_0h', 'dewpoint_forecast_local_0h', 'cloudcover_high_forecast_local_0h', 'cloudcover_low_forecast_local_0h', 'cloudcover_mid_forecast_local_0h', 'cloudcover_total_forecast_local_0h', '10_metre_v_wind_component_forecast_local_0h', 'direct_solar_radiation_forecast_local_0h', 'surface_solar_radiation_downwards_forecast_local_0h', 'snowfall_forecast_local_0h', 'total_precipitation_forecast_local_0h', 'hours_ahead_forecast_48h', 'cloudcover_low_forecast_48h', 'cloudcover_mid_forecast_48h', 'cloudcover_total_forecast_48h', 'direct_solar_radiation_forecast_48h', 'surface_solar_radiation_downwards_forecast_48h', 'hours_ahead_forecast_local_48h', 'temperature_forecast_local_48h', 'dewpoint_forecast_local_48h', 'cloudcover_low_forecast_local_48h', 'cloudcover_mid_forecast_local_48h', 'cloudcover_total_forecast_local_48h', '10_metre_v_wind_component_forecast_local_48h', 'direct_solar_radiation_forecast_local_48h', 'surface_solar_radiation_downwards_forecast_local_48h', 'temperature_forecast_168h', 'dewpoint_forecast_168h', 'cloudcover_mid_forecast_168h', '10_metre_u_wind_component_forecast_168h', '10_metre_v_wind_component_forecast_168h', 'direct_solar_radiation_forecast_168h', 'surface_solar_radiation_downwards_forecast_168h', 'temperature_forecast_local_168h', 'dewpoint_forecast_local_168h', 'cloudcover_high_forecast_local_168h', 'cloudcover_low_forecast_local_168h', 'cloudcover_total_forecast_local_168h', '10_metre_u_wind_component_forecast_local_168h', 'direct_solar_radiation_forecast_local_168h', 'surface_solar_radiation_downwards_forecast_local_168h', 'total_precipitation_forecast_local_168h', 'temperature_historical_48h', 'cloudcover_total_historical_48h', 'cloudcover_low_historical_48h', 'cloudcover_mid_historical_48h', 'cloudcover_high_historical_48h', 'shortwave_radiation', 'direct_solar_radiation_historical_48h', 'diffuse_radiation', 'temperature_historical_local_48h', 'rain_historical_local_48h', 'cloudcover_total_historical_local_48h', 'cloudcover_low_historical_local_48h', 'cloudcover_mid_historical_local_48h', 'cloudcover_high_historical_local_48h', 'shortwave_radiation_historical_local_48h', 'direct_solar_radiation_historical_local_48h', 'diffuse_radiation_historical_local_48h', 'temperature_historical_168h', 'dewpoint_historical_168h', 'snowfall_historical_168h', 'cloudcover_total_historical_168h', 'winddirection_10m_historical_168h', 'shortwave_radiation_historical_168h', 'direct_solar_radiation_historical_168h', 'diffuse_radiation_historical_168h', 'temperature_historical_local_168h', 'dewpoint_historical_local_168h', 'snowfall_historical_local_168h', 'surface_pressure_historical_local_168h', 'shortwave_radiation_historical_local_168h', 'direct_solar_radiation_historical_local_168h', 'diffuse_radiation_historical_local_168h', 'rain_historical_24h', 'snowfall_historical_24h', 'surface_pressure_historical_24h', 'cloudcover_total_historical_24h', 'cloudcover_low_historical_24h', 'cloudcover_mid_historical_24h', 'windspeed_10m_historical_24h', 'shortwave_radiation_historical_24h', 'direct_solar_radiation_historical_24h', 'diffuse_radiation_historical_24h', 'target_48h', 'target_72h', 'target_96h', 'target_120h', 'target_144h', 'target_168h', 'target_192h', 'target_216h', 'target_240h', 'target_264h', 'target_288h', 'target_312h', 'target_336h', 'target_all_type_sum_48h', 'target_all_county_type_sum_48h', 'target_all_type_sum_72h', 'target_all_county_type_sum_72h', 'target_all_type_sum_168h', 'target_all_county_type_sum_168h', 'target_all_type_sum_336h', 'target_all_county_type_sum_336h', 'target_mean', 'target_std', 'all_target_mean', 'all_target_std', 'target_ratio_168_336', 'target_ratio_48_216', 'target_ratio_72_240', 'target_ratio_48_72', 'target_all_type_sum_ratio_48_72', 'target_all_type_sum_ratio_168_336', 'target_all_county_type_sum_ratio_48_72', 'target_all_county_type_sum_ratio_168_336', 'is_country_holiday', 'feature1', 'feature2', 'skew', 'skew2', 'kurt2', 'target_mean_r_all_target_mean', 'target_std_r_all_target_std', 'target_mean_sr', 'all_target_sr', 'temperature_diff_1', 'dewpoint_diff_1', '10_metre_u_wind_component_diff_1', 'target_48h_diff_1', 'feature1_diff_1', 'target_168h_diff_1', 'feature1_mean_day', 'target_48h_diff_1_mean_day', 'target_48h_diff_1_mean_day_county', 'target_48h_diff_1_std_day', 'target_48h_diff_1_std_day_county', 'target_48h_m_target_72h', 'target_48h_m_target_96h', 'target_48h_m_target_169h', 'target_48h_m_target_336h', 'target_48h_m_target_mean', 'accel_3', 'z_score_target_48h', 'mean_target_48h_72h', 'mean_target_48h_72h_96h', 'target_48h_m_mean_target_48h_72h', 'z_score_1_target_48h', 'target_48h_m_mean_target_48h_72h_96h', 'm_std_target_48h_72h_96h', 'z_score_2_target_48h', 'diff_dsrfl', 'diff_ssrdfl', 'ratio_target48_ic', 'ratio_target_168h_ic', 'ratio_target_mean_ic', 'log_cloudcover_mid_historical_local_168h', 'log_cloudcover_mid_historical_24h', 'log_snowfall_historical_48h', 'log_rain_historical_168h', 'log_cloudcover_high_historical_48h', 'log_surface_pressure_historical_local_48h', 'log_10_metre_v_wind_component_forecast_local_0h', 'log_direct_solar_radiation_historical_24h', 'z_score_5_target_48h', 'mean_3_target_48h_sr', 'target_48h_mean_day', 'target_48h_mean_day_county', 'target_48h_min_day', 'target_48h_min_day_county', 'target_48h_max_day', 'target_48h_std_day', 'target_48h_sr_day', 'target_48h_sr_day_county', 'z_score_3_target_48h_mean_day', 'ssrdfl0_mean_day', 'ssrdfl0_mean_day_county', 'feature3', 'feature4', 'ssrdfl0_min_day', 'ssrdfl0_max_day', 'ssrdfl0_max_day_county', 'ssrdfl0_std_day', 'ssrdfl0_std_day_county', 'ssrdfl0_sr_day', 'ssrdfl0_sr_day_county']\n"
     ]
    }
   ],
   "source": [
    "l_fe = list(df_train_features.drop(columns=[\"target\", \"date\", \"hour\"], errors='ignore').columns)\n",
    "print(len(l_fe), l_fe)\n",
    "yaml.dump(l_fe, open('models/l_fe', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04eadd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:01:50.680123Z",
     "iopub.status.busy": "2024-01-22T10:01:50.679733Z",
     "iopub.status.idle": "2024-01-22T10:02:06.786161Z",
     "shell.execute_reply": "2024-01-22T10:02:06.785018Z"
    },
    "papermill": {
     "duration": 16.123386,
     "end_time": "2024-01-22T10:02:06.789497",
     "exception": false,
     "start_time": "2024-01-22T10:01:50.666111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_fet = list(df_train_features.drop(columns=[\"county\", \"is_business\", \"product_type\", 'is_consumption', \"target\"], errors='ignore').columns)\n",
    "\n",
    "df_train_features[l_fet] = df_train_features.groupby([\"county\", \"is_business\", \"product_type\", 'is_consumption'])[l_fet].ffill()\n",
    "df_train_features[l_fet] = df_train_features.groupby([\"county\", \"is_business\", \"product_type\", 'is_consumption'])[l_fet].bfill()\n",
    "\n",
    "dic_ffill_all_col = df_train_features.drop_duplicates([\"county\", \"is_business\", \"product_type\", 'is_consumption'], keep = 'last').set_index([\"county\", \"is_business\", \"product_type\", 'is_consumption'])[l_fet].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524743ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:06.813551Z",
     "iopub.status.busy": "2024-01-22T10:02:06.813105Z",
     "iopub.status.idle": "2024-01-22T10:02:06.838016Z",
     "shell.execute_reply": "2024-01-22T10:02:06.836924Z"
    },
    "papermill": {
     "duration": 0.039434,
     "end_time": "2024-01-22T10:02:06.840172",
     "exception": false,
     "start_time": "2024-01-22T10:02:06.800738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 214\n"
     ]
    }
   ],
   "source": [
    "l_ca_lstm = ['county','is_business','product_type','is_consumption']\n",
    "l_fe_lstm = [x for x in l_fe if x not in ['segment']]\n",
    "print(len(l_fe), len(l_fe_lstm))\n",
    "yaml.dump(l_fe_lstm, open('models/l_fe_lstm', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a86c91b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:06.865028Z",
     "iopub.status.busy": "2024-01-22T10:02:06.864587Z",
     "iopub.status.idle": "2024-01-22T10:02:14.622801Z",
     "shell.execute_reply": "2024-01-22T10:02:14.621344Z"
    },
    "papermill": {
     "duration": 7.773583,
     "end_time": "2024-01-22T10:02:14.625356",
     "exception": false,
     "start_time": "2024-01-22T10:02:06.851773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1486711, 165191)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "df_train_features = df_train_features.reset_index(drop = True)\n",
    "\n",
    "l_date_id = list(df_train_features.index)\n",
    "train_index = l_date_id[:int(len(l_date_id)*0.9)]\n",
    "test_index = l_date_id[int(len(l_date_id)*0.9):]\n",
    "\n",
    "print((len(train_index), len(test_index)))\n",
    "\n",
    "tr = df_train_features.iloc[train_index].reset_index(drop=True).copy()\n",
    "va = df_train_features.iloc[test_index].reset_index(drop=True).copy()   \n",
    "\n",
    "def duplicateColumns(data):\n",
    "    dupliCols=[]\n",
    "    for i in range(0,len(data.columns)):\n",
    "        col1=data.columns[i]\n",
    "        for col2 in data.columns[i+1:]:\n",
    "            if data[col1].equals(data[col2]):\n",
    "                dupliCols.append(col1+','+col2)\n",
    "    return dupliCols\n",
    "\n",
    "duplCols=duplicateColumns(va)\n",
    "print(duplCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b0619b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:14.651223Z",
     "iopub.status.busy": "2024-01-22T10:02:14.650804Z",
     "iopub.status.idle": "2024-01-22T10:02:14.671030Z",
     "shell.execute_reply": "2024-01-22T10:02:14.669905Z"
    },
    "papermill": {
     "duration": 0.036874,
     "end_time": "2024-01-22T10:02:14.673865",
     "exception": false,
     "start_time": "2024-01-22T10:02:14.636991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class lstm_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, sequence_length = sequence_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.Y = torch.tensor(Y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "        return x, self.Y[i]\n",
    "    \n",
    "class by_category_lstm_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 X : np.ndarray, \n",
    "                 Y : np.ndarray,  \n",
    "                 l_ca : list, \n",
    "                 l_fe : list, \n",
    "                 sequence_length = sequence_length):\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.l_cat = l_ca\n",
    "        self.l_fe = l_fe\n",
    "        self.l_fe_col_idx_minus_cat = [i[0] for i in enumerate(l_fe) if i[1] not in l_ca]\n",
    "        self.l_cat_col_idx = [i[0] for i in enumerate(l_fe) if i[1] in l_ca]\n",
    "        \n",
    "        #### INT for Categorical var (for pytorch)\n",
    "        self.cat = torch.tensor(X[:, self.l_cat_col_idx].astype(int))\n",
    "        self.idx = torch.tensor(np.arange(0, len(self.cat)))\n",
    "        self.fes = torch.tensor(X[:, self.l_fe_col_idx_minus_cat].astype(float)).float()\n",
    "        self.Y = torch.tensor(Y).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        fes_get = self.fes[(self.idx <= i) & (self.cat == self.cat[i]).all(axis=1)]\n",
    "        cat_get = self.cat[(self.idx <= i) & (self.cat == self.cat[i]).all(axis=1)]\n",
    "        size =  len(cat_get)\n",
    "        if size >= sequence_length - 1:\n",
    "            i_start = size - sequence_length + 1\n",
    "            fes_get = fes_get[i_start:(i + 1), :]\n",
    "            cat_get = cat_get[i_start:(i + 1), :]\n",
    "\n",
    "        else:\n",
    "            padding = fes_get[0].repeat(self.sequence_length - size - 1, 1)\n",
    "            fes_get = fes_get[0:(i + 1), :]\n",
    "            fes_get = torch.cat((padding, fes_get), 0)\n",
    "            \n",
    "            padding = cat_get[0].repeat(self.sequence_length - size - 1, 1)\n",
    "            cat_get = cat_get[0:(i + 1), :]\n",
    "            cat_get = torch.cat((padding, cat_get), 0)\n",
    "        return torch.cat((cat_get, fes_get), 1), self.Y[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f882ed21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:14.698523Z",
     "iopub.status.busy": "2024-01-22T10:02:14.698093Z",
     "iopub.status.idle": "2024-01-22T10:02:14.706306Z",
     "shell.execute_reply": "2024-01-22T10:02:14.705459Z"
    },
    "papermill": {
     "duration": 0.023001,
     "end_time": "2024-01-22T10:02:14.708475",
     "exception": false,
     "start_time": "2024-01-22T10:02:14.685474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.00001):\n",
    "        self.best_model = None\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        \n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def early_stop(self, validation_loss, model):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e1e749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:14.733293Z",
     "iopub.status.busy": "2024-01-22T10:02:14.732860Z",
     "iopub.status.idle": "2024-01-22T10:02:14.745886Z",
     "shell.execute_reply": "2024-01-22T10:02:14.744511Z"
    },
    "papermill": {
     "duration": 0.02801,
     "end_time": "2024-01-22T10:02:14.748165",
     "exception": false,
     "start_time": "2024-01-22T10:02:14.720155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tr_loop(dataloader, model, loss_fn, optimizer, shortcut=0):\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss  = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss  += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if pred.std() < 0.000001:\n",
    "            print(\"WARNING: std() is zero, stopping\")\n",
    "            break\n",
    "        if shortcut > 0 and batch == shortcut:\n",
    "            num_batches = shortcut\n",
    "            break \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return total_loss#avg_loss\n",
    "\n",
    "def va_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step(total_loss)\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return total_loss#avg_loss\n",
    "        \n",
    "def predict(data_loader, model):\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "    return output.detach().cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d856183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:02:14.772636Z",
     "iopub.status.busy": "2024-01-22T10:02:14.772189Z",
     "iopub.status.idle": "2024-01-22T10:03:00.479766Z",
     "shell.execute_reply": "2024-01-22T10:03:00.478588Z"
    },
    "papermill": {
     "duration": 45.723249,
     "end_time": "2024-01-22T10:03:00.482749",
     "exception": false,
     "start_time": "2024-01-22T10:02:14.759500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_np = tr[l_fe_lstm].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "va_np = va[l_fe_lstm].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "mms = sklearn.preprocessing.MinMaxScaler()\n",
    "mms.fit(tr_np)\n",
    "tr_np = mms.transform(tr_np)\n",
    "va_np = mms.transform(va_np)\n",
    "joblib.dump(mms, f'models/MinMaxScaler.l_fe')\n",
    "\n",
    "tr_y_np = tr['target'].values\n",
    "va_y_np = va['target'].values\n",
    "mmst = sklearn.preprocessing.MinMaxScaler()\n",
    "mmst.fit(np.array(tr_y_np).reshape(-1, 1))\n",
    "\n",
    "tr_y_np = mmst.transform(np.array(tr_y_np).reshape(-1, 1))\n",
    "va_y_np = mmst.transform(np.array(va_y_np).reshape(-1, 1))\n",
    "joblib.dump(mmst, f'models/MinMaxScaler.target')\n",
    "\n",
    "\n",
    "tr_py = by_category_lstm_Dataset(tr_np, tr_y_np, l_ca_lstm, l_fe_lstm)\n",
    "va_py = by_category_lstm_Dataset(va_np, va_y_np, l_ca_lstm, l_fe_lstm)\n",
    "\n",
    "tr_dataloader = torch.utils.data.DataLoader(tr_py, batch_size=batch_size, shuffle=False)\n",
    "va_dataloader = torch.utils.data.DataLoader(va_py, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cec622b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:03:00.507948Z",
     "iopub.status.busy": "2024-01-22T10:03:00.507568Z",
     "iopub.status.idle": "2024-01-22T10:03:00.535974Z",
     "shell.execute_reply": "2024-01-22T10:03:00.534996Z"
    },
    "papermill": {
     "duration": 0.044078,
     "end_time": "2024-01-22T10:03:00.538424",
     "exception": false,
     "start_time": "2024-01-22T10:03:00.494346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class condLSTM(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 l_ca : list, \n",
    "                 l_fe : list,\n",
    "                 hidden_size=256, \n",
    "                 output_size=1, \n",
    "                 num_layers= 3, \n",
    "                 bidir = True,\n",
    "                 Verbose = True\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.Attention_on_hidden_layer = True\n",
    "        self.Verbose = Verbose \n",
    "        self.l_ca_size = len(l_ca)\n",
    "        self.l_fe_minus_cat_size = len([i[1] for i in enumerate(l_fe) if i[1] not in l_ca])\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size \n",
    "        \n",
    "        self.mult_bidir = 2 if bidir else 1\n",
    "        self.cond_c = torch.nn.Linear(in_features = self.l_ca_size, out_features = self.hidden_size * self.num_layers * self.mult_bidir)\n",
    "        self.cond_h = torch.nn.Linear(in_features = self.l_ca_size, out_features = self.hidden_size * self.num_layers * self.mult_bidir)\n",
    "        \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=self.l_fe_minus_cat_size, out_channels= self.l_fe_minus_cat_size, padding =2, kernel_size=5)        \n",
    "        self.norm1 = torch.nn.BatchNorm1d(self.l_fe_minus_cat_size)\n",
    "        self.lstm = torch.torch.nn.LSTM(input_size = self.l_fe_minus_cat_size, hidden_size = self.hidden_size, \n",
    "                                        num_layers = self.num_layers, \n",
    "                                        bidirectional  = bidir, dropout=0.05, batch_first=True)\n",
    "\n",
    "        if self.Attention_on_hidden_layer : \n",
    "            self.norm2 = torch.nn.BatchNorm1d(self.hidden_size * self.mult_bidir)\n",
    "            self.lrelu1 = torch.nn.LeakyReLU()\n",
    "            self.attn = torch.nn.MultiheadAttention(embed_dim=self.hidden_size * self.mult_bidir, num_heads=16)\n",
    "        else :\n",
    "            self.norm2 = torch.nn.BatchNorm1d(self.hidden_size * self.mult_bidir *2)\n",
    "            self.lrelu1 = torch.nn.LeakyReLU()\n",
    "            self.attn = torch.nn.MultiheadAttention(embed_dim=self.hidden_size * self.mult_bidir*2, num_heads=16)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(self.l_fe_minus_cat_size)\n",
    "        self.lrelu2 = torch.nn.LeakyReLU()\n",
    "        self.linear1 = torch.nn.Linear(in_features = self.hidden_size * self.mult_bidir, out_features=int(hidden_size/2) * self.mult_bidir)\n",
    "        self.linear2 = torch.nn.Linear(in_features=int(hidden_size/2) * self.mult_bidir, out_features=1)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        if self.Verbose : print(f'x : {x.shape}')\n",
    "        if self.Verbose : print(f'x[:, 0, :self.cat_size ] : {x[:, 0, :self.l_ca_size ].shape}')\n",
    "        cat_c = self.cond_c(x[:, 0, :self.l_ca_size])\n",
    "        cat_h = self.cond_h(x[:, 0, :self.l_ca_size])\n",
    "        c0 = torch.reshape(cat_c, (self.num_layers * self.mult_bidir, -1, self.hidden_size))\n",
    "        h0 = torch.reshape(cat_h, (self.num_layers * self.mult_bidir, -1, self.hidden_size))\n",
    "        #h0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        #c0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        #torch.nn.init.xavier_normal_(h0)\n",
    "        #torch.nn.init.xavier_normal_(c0)\n",
    "        # Xavier/Glorot Initialization: for Activation => None, hyperbolic Tan (tanh), Logistic(sigmoid), softmax.\n",
    "        # He Initialization: for Activation => Rectified Linear activation unit(ReLU) and Variants.\n",
    "        # LeCun Initialization: for Activation => Scaled Exponential Linear Unit(SELU)\n",
    "        if self.Verbose :print(f'c0 : {c0.shape}')\n",
    "        if self.Verbose :print(f'h0 : {h0.shape}')\n",
    "        \n",
    "        \n",
    "        x1 = x[:, :, self.l_ca_size:].transpose(1, 2) \n",
    "        if self.Verbose :print(f'x after transpose 1 : {x1.shape}')\n",
    "        x1 = self.conv1d(x1)    \n",
    "        x1 = self.norm1(x1)\n",
    "        if self.Verbose :print(f'x after conv1d : {x1.shape}')\n",
    "        #Turn (batch_size x hidden_size x seq_len) back into (seq_len x batch_size x hidden_size) for LSTM\n",
    "        x1 = x1.transpose(1, 2)\n",
    "        \n",
    "        x, (hn, _) = self.lstm(x1, (h0, c0))\n",
    "        if self.Verbose : print(f'x after lstm : {x.shape}')\n",
    "        if self.Verbose : print(f'hn after lstm : {hn.shape}')\n",
    "        if self.mult_bidir == 2:\n",
    "            hn = torch.cat((hn[-1,:, :], hn[-2,:, :]),1)\n",
    "            x = torch.cat((x[:,-1, :], x[:,-2, :]),1)\n",
    "        else :\n",
    "            hn = hn[-1,:, :]\n",
    "            x = x[:, -1, :]\n",
    "        if self.Verbose : print(f'x after filter / before attn: {x.shape}')\n",
    "        if self.Verbose : print(f'hn after filter / before attn: {hn.shape}')\n",
    "        if self.Attention_on_hidden_layer : \n",
    "            hn = self.norm2(hn)            \n",
    "            hn = self.lrelu1(hn)\n",
    "            x, weight = self.attn(hn, hn, hn)\n",
    "        else:\n",
    "            x = self.norm2(x)\n",
    "            x = self.lrelu1(x)\n",
    "            x, weight = self.attn(x, x, x)\n",
    "        if self.Verbose : print(f'x after attn : {x.shape}')\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.linear1(x)\n",
    "        if self.Verbose : print(f'x after linear1 : {x.shape}')\n",
    "        x = self.linear2(x)\n",
    "        if self.Verbose : print(f'x after linear2 : {x.shape}'); self.Verbose = False\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cffe5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T10:03:00.563423Z",
     "iopub.status.busy": "2024-01-22T10:03:00.562324Z"
    },
    "papermill": {
     "duration": 312.394117,
     "end_time": "2024-01-22T10:08:12.943772",
     "exception": false,
     "start_time": "2024-01-22T10:03:00.549655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([10240, 6, 214])\n",
      "x[:, 0, :self.cat_size ] : torch.Size([10240, 4])\n",
      "c0 : torch.Size([6, 10240, 256])\n",
      "h0 : torch.Size([6, 10240, 256])\n",
      "x after transpose 1 : torch.Size([10240, 210, 6])\n",
      "x after conv1d : torch.Size([10240, 210, 6])\n",
      "x after lstm : torch.Size([10240, 6, 512])\n",
      "hn after lstm : torch.Size([6, 10240, 256])\n",
      "x after filter / before attn: torch.Size([10240, 1024])\n",
      "hn after filter / before attn: torch.Size([10240, 512])\n",
      "x after attn : torch.Size([10240, 512])\n",
      "x after linear1 : torch.Size([10240, 256])\n",
      "x after linear2 : torch.Size([10240, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2023)\n",
    "model = condLSTM(l_ca_lstm, l_fe_lstm).float()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.5, verbose=True)\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0.0001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "libc.malloc_trim(0)\n",
    "gc.collect()\n",
    "\n",
    "history = pd.DataFrame([], columns=[\"epoch\", \"train_loss\", \"test_loss\", \"lr\"])\n",
    "for epoch in range(n_estimators):\n",
    "    tr_loss = tr_loop(tr_dataloader, model, loss_fn, optimizer, shortcut=1)                 \n",
    "    va_loss = va_loop(va_dataloader, model, loss_fn)\n",
    "    print(f\"Epoch {epoch+1:>3d}, Train : {tr_loss:>5f}, Valid : {va_loss:>5f}\")\n",
    "    if early_stopper.early_stop(va_loss, model):  \n",
    "        model = early_stopper.get_best_model()\n",
    "        break\n",
    "    history.loc[len(history),:] = [epoch+1, tr_loss, va_loss, optimizer.param_groups[0]['lr']]\n",
    "\n",
    "joblib.dump(model, f'models/model.lstm.jolib')\n",
    "torch.save(model.state_dict(), f'models/model.state_dict')\n",
    "torch.save(model, 'models/model.full')\n",
    "pred = predict(va_dataloader, model)     \n",
    "pred = mmst.inverse_transform(pred.reshape(-1, 1)).ravel()\n",
    "mae_score = np.sum(np.abs((pred - va['target'].values)))/len(va['target'].values)\n",
    "print(f'val mae {mae_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725176d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-22T07:29:02.594353Z",
     "iopub.status.idle": "2024-01-22T07:29:02.594908Z",
     "shell.execute_reply": "2024-01-22T07:29:02.594693Z",
     "shell.execute_reply.started": "2024-01-22T07:29:02.594668Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "va['error'] = target - pred\n",
    "va['pred'] = pred\n",
    "va['error2'] = va['error'].abs()\n",
    "print(va['error2'].mean() , va['error2'].max())\n",
    "    \n",
    "mae_score = np.sum(np.abs((pred  - target)))/len(pred)\n",
    "print(f'val mae {mae_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f10192",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-22T07:29:02.596199Z",
     "iopub.status.idle": "2024-01-22T07:29:02.596880Z",
     "shell.execute_reply": "2024-01-22T07:29:02.596579Z",
     "shell.execute_reply.started": "2024-01-22T07:29:02.596546Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(va, x = 'pred', y = 'target')\n",
    "plt.show()\n",
    "corr_error = va[l_fe + ['error', 'target']].corr()\n",
    "display(corr_error['error'].abs().nlargest(15))\n",
    "display(corr_error['target'].abs().nlargest(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08349190",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-22T07:29:02.600101Z",
     "iopub.status.idle": "2024-01-22T07:29:02.600588Z",
     "shell.execute_reply": "2024-01-22T07:29:02.600380Z",
     "shell.execute_reply.started": "2024-01-22T07:29:02.600359Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "limit = 3090 #va['error2'].mean()\n",
    "display(va[va['error2']> limit])\n",
    "sns.scatterplot(va[va['error2']> limit], x = 'pred', y = 'target')\n",
    "plt.show()\n",
    "corr_error = va[va['error2']> limit][l_fe + ['error', 'target']].corr()\n",
    "display(corr_error['error'].abs().nlargest(15))\n",
    "display(corr_error['target'].abs().nlargest(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9b2f2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-22T07:29:02.602698Z",
     "iopub.status.idle": "2024-01-22T07:29:02.603343Z",
     "shell.execute_reply": "2024-01-22T07:29:02.603060Z",
     "shell.execute_reply.started": "2024-01-22T07:29:02.603029Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size=len(l_fe), hidden_size=hidden_size, output_size=1, num_layers= num_layers, bidir = True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size \n",
    "        self.test = True\n",
    "        self.mult_bidir = 2 if bidir else 1\n",
    "        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers , dropout=0.05, batch_first=True, bidirectional  = bidir)\n",
    "        self.linear1 = torch.nn.Linear(in_features =hidden_size * self.mult_bidir, out_features=int(hidden_size/2) * self.mult_bidir)\n",
    "        self.linear2 = torch.nn.Linear(in_features=int(hidden_size/2) * self.mult_bidir, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.test : print(f'x : {x.shape}')\n",
    "        h0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        torch.nn.init.xavier_normal_(h0)\n",
    "        torch.nn.init.xavier_normal_(c0)\n",
    "        if self.test :print(f'h0 : {h0.shape}')\n",
    "        if self.test :print(f'c0 : {c0.shape}')\n",
    "        x, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        if self.test : print(f'x after lstm : {x.shape}')\n",
    "        if self.test : print(f'hn after lstm : {hn.shape}')\n",
    "        x = x[:, -1, :]\n",
    "        if self.test : print(f'x after filter : {x.shape}')\n",
    "        x = self.linear1(x)\n",
    "        if self.test : print(f'x after linear1 : {x.shape}')\n",
    "        x = self.linear2(x)\n",
    "        if self.test : print(f'x after linear2 : {x.shape}'); self.test = False\n",
    "        return x\n",
    "\n",
    "class LSTMwithAttention(torch.nn.Module):\n",
    "    def __init__(self, input_size=len(l_fe), hidden_size=hidden_size, output_size=1, num_layers= num_layers, bidir = True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size \n",
    "        self.test = True\n",
    "        self.mult_bidir = 2 if bidir else 1\n",
    "        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers , dropout=0.05, batch_first=True, bidirectional  = bidir)\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim=hidden_size * self.mult_bidir, num_heads=5)\n",
    "        self.linear1 = torch.nn.Linear(in_features=hidden_size * self.mult_bidir, out_features=int(hidden_size/2) * self.mult_bidir)\n",
    "        self.linear2 = torch.nn.Linear(in_features=int(hidden_size/2) * self.mult_bidir, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.test : print(f'x : {x.shape}')\n",
    "        h0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        torch.nn.init.xavier_normal_(h0)\n",
    "        torch.nn.init.xavier_normal_(c0)\n",
    "        if self.test :print(f'h0 : {h0.shape}')\n",
    "        if self.test :print(f'c0 : {c0.shape}')\n",
    "        x, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        if self.test : print(f'x after lstm : {x.shape}')\n",
    "        if self.test : print(f'hn after lstm : {hn.shape}')\n",
    "        if self.mult_bidir == 2:\n",
    "            hn = torch.cat((hn[-1,:, :], hn[-2,:, :]),1)\n",
    "        else :\n",
    "            hn = hn[-1,:, :]\n",
    "        if self.test : print(f'hn after filter : {hn.shape}')\n",
    "        x, weight = self.attn(hn, hn, hn)\n",
    "        if self.test : print(f'x after attn : {x.shape}')\n",
    "        x = self.linear1(x)\n",
    "        if self.test : print(f'x after linear1 : {x.shape}')\n",
    "        x = self.linear2(x)\n",
    "        if self.test : print(f'x after linear2 : {x.shape}'); self.test = False\n",
    "        return x\n",
    "    \n",
    "class convLSTMwithAttention(torch.nn.Module):\n",
    "    def __init__(self, input_size=len(l_fe), hidden_size=hidden_size, output_size=1, num_layers= num_layers, bidir = True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size \n",
    "        self.test = True\n",
    "        \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=input_size, out_channels=hidden_size, padding =2, kernel_size=5)\n",
    "        #self.conv1d = torch.nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=1)\n",
    "        self.mult_bidir = 2 if bidir else 1\n",
    "        self.lstm = torch.nn.LSTM(input_size = hidden_size, hidden_size = hidden_size, num_layers = num_layers , dropout=0.05, batch_first=True, bidirectional  = bidir)\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim=hidden_size * self.mult_bidir, num_heads=5)\n",
    "        self.linear1 = torch.nn.Linear(in_features=hidden_size * self.mult_bidir, out_features=int(hidden_size/2) * self.mult_bidir)\n",
    "        self.linear2 = torch.nn.Linear(in_features=int(hidden_size/2) * self.mult_bidir, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.test : print(f'x : {x.shape}')\n",
    "        # Turn (seq_len x batch_size x input_size) into (batch_size x input_size x seq_len) for CNN\n",
    "        x = x.transpose(1, 2) \n",
    "        if self.test :print(f'x after transpose 1 : {x.shape}')\n",
    "        x = self.conv1d(x)    \n",
    "        if self.test :print(f'x after conv1d : {x.shape}')\n",
    "        #Turn (batch_size x hidden_size x seq_len) back into (seq_len x batch_size x hidden_size) for LSTM\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.test :print(f'x after transpose 2 : {x.shape}')\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers * self.mult_bidir, x.shape[0], self.hidden_size).requires_grad_()\n",
    "        torch.nn.init.xavier_normal_(h0)\n",
    "        torch.nn.init.xavier_normal_(c0)\n",
    "        if self.test :print(f'h0 : {h0.shape}')\n",
    "        if self.test :print(f'c0 : {c0.shape}')\n",
    "        x, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        if self.test : print(f'x after lstm : {x.shape}')\n",
    "        if self.test : print(f'hn after lstm : {hn.shape}')\n",
    "        if self.mult_bidir == 2:\n",
    "            hn = torch.cat((hn[-1,:, :], hn[-2,:, :]),1)\n",
    "        else :\n",
    "            hn = hn[-1,:, :]\n",
    "        if self.test : print(f'hn after filter : {hn.shape}')\n",
    "        x, weight = self.attn(hn, hn, hn)\n",
    "        if self.test : print(f'x after attn : {x.shape}')\n",
    "        x = self.linear1(x)\n",
    "        if self.test : print(f'x after linear1 : {x.shape}')\n",
    "        x = self.linear2(x)\n",
    "        if self.test : print(f'x after linear2 : {x.shape}'); self.test = False\n",
    "        return x "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 471.01322,
   "end_time": "2024-01-22T10:08:14.079772",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-22T10:00:23.066552",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
